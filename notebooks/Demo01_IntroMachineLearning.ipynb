{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO #1 Introdução a Aprendizagem de Máquina\n",
    "\n",
    "## Demonstração das ferramentas e do _pipeline_ de _Machine Learning_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ferramentas\n",
    "\n",
    "Inicialmente vamos instalar as bibliotecas que serão utilizadas. Depois de instalado `Python`, instalamos:\n",
    "\n",
    "- `numpy`: Ferramentas numéricas\n",
    "- `scikit-learn`: Algoritimos de classificação e _helpers_ para ML\n",
    "- `tensorflow`: Biblioteca para grafos de fluxo computacional\n",
    "- `keras`: API de alto nível para Deep Learning\n",
    "- `matplotlib`: Biblioteca para plotar imagens e gráficos\n",
    "\n",
    "Com o comando:\n",
    "\n",
    "```\n",
    "pip install numpy scikit-learn tensorflow keras matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testamos a instalação importando as ferramentas\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1º Parte do Pipeline: Dados de Entrada\n",
    "\n",
    "Vamos usar um dataset chamado MNIST; ele é composto de 60 mil imagens dígitos escritos manualmente, preparados para serem classificados entre 0, 1, 2... 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(\"DATA: \", x_train)\n",
    "print(\"LABELS: \", y_train)\n",
    "print(\"Data Shape: \", x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some samples from the dataset\n",
    "plt.imshow(np.concatenate((x_train[0:5]), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2º Parte do Pipeline: Extração de Características\n",
    "\n",
    "Precisamos extraír características dessas imagens, alguma sugestão? Vamos criar uma função implementando nossa ideia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mean_pixel(x):\n",
    "    mean_pixel = np.mean(x)\n",
    "    return mean_pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first reshape our images so that they become vectors\n",
    "x_train = x_train.reshape((-1, 784))\n",
    "x_test = x_test.reshape((-1, 784))\n",
    "\n",
    "# extract features from each sample\n",
    "training_features = np.array([[extract_mean_pixel(x)] for x in x_train])\n",
    "testing_features = np.array([[extract_mean_pixel(x)] for x in x_test])\n",
    "\n",
    "print(training_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3º Parte do Pipeline: Algotitimo de Classificação\n",
    "\n",
    "Para demonstração, vamos arbitrariamente escolher o algoritimo **SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC as AClassificationAlgorithm\n",
    "\n",
    "classification_algorithm = AClassificationAlgorithm()\n",
    "\n",
    "# train on our training set\n",
    "num_samples = 10000 # number of samples to use for training\n",
    "classification_algorithm.fit(training_features[:num_samples], y_train[:num_samples])\n",
    "\n",
    "# and we test on the testing set\n",
    "expected_result = y_test\n",
    "predicted_result = classification_algorithm.predict(testing_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Última parte: Custo, Risco e Generalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-1 Loss\n",
    "zero_one_loss = expected_result == predicted_result\n",
    "print(\"0-1 Loss: \", zero_one_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_one_risk = np.sum(zero_one_loss)/len(testing_features)\n",
    "print(\"0-1 Risk: \", zero_one_risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas de que tipo de risco estamos falando aqui? **Risco em dados de teste**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# risk on training data\n",
    "training_data_loss = classification_algorithm.predict(training_features[:num_samples]) == y_train[:num_samples]\n",
    "training_data_risk = np.sum(training_data_loss)/num_samples\n",
    "print(\"Training Data Risk: \", training_data_risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumindo que nossos dados de teste são um bom chute para os dados reais, podemos calcular a generalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate generalization of our classifier\n",
    "approximate_real_risk = zero_one_risk\n",
    "generalization = np.abs(training_data_risk - approximate_real_risk)\n",
    "print(\"Generalization: \", generalization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
